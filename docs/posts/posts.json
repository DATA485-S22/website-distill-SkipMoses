[
  {
    "path": "posts/2022-05-31-causal-inference/",
    "title": "Causal Inference",
    "description": "Notes on Causal Inference following the course by Brady Neal.",
    "author": [
      {
        "name": "Skip Moses",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-05-31",
    "categories": [],
    "contents": "\r\nIn this post we preview the course on Causal Inference. This post\r\nwill give a brief overview of the main concepts.\r\nWhat is causal inference?\r\nInferring the effects of any\r\ntreatment/policy/intervention/ect.\r\nExamples:\r\nEffect of treatment on a disease\r\nEffect of climate change policy on emissions\r\nEffect of social media on mental health\r\nIn general, effect of \\(X\\) on\r\n\\(Y\\).\r\n\r\nMotivating Example\r\nSuppose we have some disease we are tying to treat with treatment A\r\nand treatment B. Our only goal is minimizing death. Suppose treatment B\r\nis much more scarce than treatment A.\r\nTreatment T: A (0) and B (1)\r\nCondition C: mild (0) or severe (1)\r\nOutcome Y : alive (0) dead (1)\r\nData at Treatment Level\r\nTreatment\r\nTotal\r\nA\r\n240/1500 16%\r\nB\r\n105/550 19%\r\nNote this column is just \\(\\mathbb{E}[Y\r\n\\vert T]\\)\r\nOn average 16% (240 out of 1500) died after receiving treatment\r\nA.\r\nOn average 19% (105 out of 550) died after receiving treatment\r\nB.\r\nIt appears treatment A is the best option, because fewer patients\r\nthat received treatment A died.\r\nData at the Condition Level\r\nTreatment\r\nMild\r\nSevere\r\nTotal\r\nA\r\n210/1400 15%\r\n30/100 30%\r\n240/1500 16%\r\nB\r\n5/50 10%\r\n100/500 20%\r\n105/550 19%\r\nNote the two new columns are \\(\\mathbb{E}[Y \\vert T,C]\\).\r\nNow that we have conditioned on condition we see our conclusion\r\nis flipped. Fewer patients died that received treatment B in both mild\r\nand severe patients.\r\nThis is Simpson’s Paradox: Our conclusions seem to depend on how\r\nwe partition our data.\r\nWe can think of these numbers the following way:\r\n\\[\r\n  \\frac{1400}{1500}(0.15) + \\frac{100}{1500}(0.30) = 0.16\r\n\\]\r\n\\[\r\n  \\frac{50}{550}(0.10) + \\frac{500}{550}(0.20) = 0.19\r\n\\]\r\nNow, the fractions are like weights on our percentages. We see\r\nthe 19% of patients that received treatment B had a severe\r\ncondition.\r\nLikewise, the most patients that received treatment A only had a\r\nmild condition.\r\nWhich treatment should you\r\nchoose?\r\nThe answer will depend on the causal structure of the\r\nproblem.\r\nTreatment and Outcome depend on\r\nConditionIn this case, treatment B is the correct choice.\r\nSince treatment B was scarcer, doctors administer the more readily\r\navailable treatment A.\r\nBut if a more severe case is identified the doctor administers\r\ntreatment B.\r\n\r\nCondition and Outcome depend on\r\nTreatmentIn this case, treatment A is the correct choice.\r\nBecause of the scarcity of treatment B, it is possible you will have\r\nto wait for treatment B.\r\nAlternatively, you don’t have to wait to receive treatment A.\r\nA patient with a mild condition, might progress to a severe\r\ncondition because they have to wait.\r\nIn order to account for effect of treatment through condition, we\r\nconsider the total numbers.\r\n\r\nCorrelation does not imply\r\ncausation\r\nObservations can be correlated by chance, or if there is a common\r\ncause of both.\r\nSuppose sleeping with shoes on is highly correlated with waking up\r\nwith a headache.\r\nYou might conclude that sleeping with your shoes on causes a\r\nheadache in the morning.\r\nHowever, a large majority of those who slept with their shoes on\r\nalso drank the night before.\r\nIn this example drinking the night before is counfounding the\r\nassociation between sleeping with shoes on and waking up with a\r\nheadache.\r\n\r\nTotal association (e.g. correlation) is a mixture of causal and\r\nconfounding association.\r\nWhat does imply causation?\r\nPotential Outcomes\r\nSuppose you have a headache, and you know if you take a pill your\r\nheadache will go away; if you don’t your headache will not go away.\r\nYou might say the pill causes the headache to go away.\r\nIf your headache goes away without taking the pill, you might\r\nconclude the pill didn’t do anything.\r\n\r\nThe causal effect of the treatment on the outcome is defined to be\r\ndifference between potential outcomes. \\[Y_i(1) - Y_i(0)\\]\r\nFundamental problem of\r\ncausal inference\r\nSuppose you do not take the pill, then \\(Y_i(0)\\) is the Factual.\r\nThe problem is we cannot compute the Counterfactual, \\(Y_i(1)\\).\r\nTherefore, we cannot compute the causal effect.\r\nWork around (Average\r\nTreatment Effect ATE)\r\nLeverage linearity of expectation to\r\nDenote the individual treatment effect (ITE) by \\(Y_i(1)-Y_i(0)\\).\r\nThe ATE is \\(\\mathbb{E}[Y_i(1) -\r\nY_i(0)] = \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)]\\).\r\nNote, potential outcomes are not actual outcomes, so\r\n\\[\r\n  \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] \\neq \\mathbb{E}[Y\\vert T=1] -\r\n\\mathbb{E}[Y\\vert T = 0]\r\n\\] The left hand side is causal, while the right hand side is\r\ncausal and confounding.\r\nThis is where randomized trials come in. This allows us to remove\r\nany causal relationship between treatment and condition.\r\nWhen there is no confounding: \\[\r\n\\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] = \\mathbb{E}[Y\\vert T=1] -\r\n\\mathbb{E}[Y\\vert T = 0]\r\n\\]\r\nRandomization is very powerful, because it also removes any\r\ncausal effects of unobserved variables.\r\nObservational Sudies\r\nCan’t always be randomized.\r\nRandomization could be unethical/infeasible/impossible.\r\n\r\nHow do we measure causal effect in observational studies?\r\nWe adjust/control for the right variables \\(W\\).\r\nIf \\(W\\) is a sufficient\r\nadjustment set, we have\r\n\r\n\\[\r\n  \\mathbb{E}[Y(t) \\vert W = w] := \\mathbb{E}[Y\\vert do(T=t), W =w] =\r\n\\mathbb{E}[Y\\vert t,w]\r\n\\] - This still depends on \\(w\\), so we take the marginal\r\n\\[\r\n  \\mathbb{E}[Y(t)] := \\mathbb{E}[Y\\vert do(T=t)] =\r\n\\mathbb{E}_W\\mathbb{E}[Y\\vert t, W]\r\n\\]\r\nExample\r\nTreatment\r\nMild\r\nSevere\r\nTotal\r\nA\r\n210/1400 15%\r\n30/100 30%\r\n240/1500 16%\r\nB\r\n5/50 10%\r\n100/500 20%\r\n105/550 19%\r\nHere are sufficient adjustment is Condition.\r\n\\[\r\n  \\mathbb{E}[Y\\vert do(T=t)] = \\mathbb{E}_W\\mathbb{E}[Y\\vert t, C] =\r\n\\sum_{c\\in C} \\mathbb{E}[Y\\vert t,c]P(c)\r\n\\]\r\n\\[\r\n  \\frac{1450}{2050}(0.15) + \\frac{600}{2050}(0.30) \\approx 0.194\r\n\\]\r\n\\[\r\n  \\frac{1450}{2050}(0.10) + \\frac{600}{2050}(0.20) \\approx 0.129\r\n\\]\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-31T14:40:08-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-11-gsp/",
    "title": "Graph Signal Processing Introduction",
    "description": "A brief overview of an algorithm for leanring graph laplacians from smooth signal representations over a network.",
    "author": [
      {
        "name": "Skip Moses",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-05-11",
    "categories": [],
    "contents": "\r\nIntroduction\r\nData often has an underlying structure or geometry that can be modeled as a signal on the vertices of a weighted, undirected graph. There are several analogies between traditional signal processing and algebraic graph theory that translates many of the tools of discrete signal processing such as spectral analysis of multichannel signals, system transfer function, digital filter design, parameter estimation, and optimal denoising. Historically, graph signal processing (GSP) has focused on modeling smooth signals on a graph, but the increase in availabilty of abstract data sets has led to progress in learning a valid graph given a set of signals.\r\nPreliminaries\r\nA weighted, undirected graph is a triple \\(G = (V, E, \\omega)\\) of two sets \\(V = \\{1, \\ldots, |V| = N \\}\\) and \\(E \\subset V \\times V\\) and a weighting function \\(\\omega(i,j) = \\omega_{i,j}\\) that assigns a nonnegative real number to each edge. We can represent a graph by its adjacency matrix \\(A\\) where \\(A_{i,j} = \\omega_{i,j}\\) if \\((i,j) \\in E\\) and \\(0\\) otherwise. A signal on a graph \\(G\\) is a function \\(f: V \\rightarrow \\mathbb{R}\\) that can be represented as vector \\(x \\in \\mathbb{R}^N\\). The Laplacian of a graph is the matrix \\(L = D - A\\) where \\(D\\) is the degree matrix. The Laplacian acts as a difference operator on signals via it’s quadratic form\r\n\\[\\begin{equation}\r\n    x^TLx = \\sum_{(i,j)\\in E}A_{i,j}(x_j - x_i)^2 \r\n\\end{equation}\\]\r\nThe Laplacian is positive semi definite, so it has a complete set of orthornormal eigenvectors, and real non negative eigenvalues. Thus, we can diagonalize \\(L = \\chi^T\\Lambda \\chi\\) where \\(\\Lambda\\) is the diagonal matirx of eigenvalues and \\(\\chi\\) is associated matrix of eigenvectors.\r\nNote that the quadratic form above is minimized when adjacent vertices have identical signal values. This makes program well suited for measuring the smoothness of a signal on a graph. We can cast the problem of learning a graph by the optimization problem found in Kalofolias (2016)\r\n\\[\\begin{equation} \r\n    \\begin{aligned}\r\n        \\min_L       & \\phantom{..} \\text{tr}(Y^TLY) + f(L),\\\\\r\n        \\textrm{s.t.}& \\quad \\text{tr}(L) = N,\\\\\r\n        & \\quad L_{i,j} = L_{j,i} \\leq 0, \\phantom{..} i \\neq j, \\\\\r\n        & \\quad L\\cdot \\textbf{1} = \\textbf{0}\r\n    \\end{aligned}\r\n\\end{equation}\\]\r\nGraph Learning Model\r\nDong et al. propose a modified Factor Analysis model in Dong et al. (2016) for learning a valid graph laplacian. The model proposed is given by \\[x = \\chi h + \\mu_x + \\epsilon\\] where \\(h \\in \\mathbb{R}^N\\) is the latent variable, \\(\\mu_x \\in \\mathbb{R}^N\\) mean of \\(x\\). The noise term \\(\\epsilon\\) is assumed to follow a multivariate Gaussian distribution with mean zero and covariance \\(\\sigma_\\epsilon^2I_N\\). The key difference from traditional factor analysis is the choice of \\(\\chi\\) as a valid eigenvector matrix for a graph laplacian. Finding a maximum apriori estimate of \\(h\\) reduces to solving the optimization problem\r\n\\[\\begin{equation}\r\n    \\begin{aligned}\r\n        \\min_{L \\in \\mathbb{R}^{N\\times N}, \\phantom{..} Y \\in \\mathbb{R}^{N\\times P}} ||X&-Y||_F^2 + \\alpha \\text{tr}(Y^TLY) + \\beta||L||_F^2 \\\\\r\n        \\textrm{s.t.}& \\quad \\text{tr}(L) = N, \\\\\r\n        &\\quad L_{i,j} = L_{j,i} \\leq 0, \\phantom{..} i \\neq j, \\\\\r\n        &\\quad L\\cdot \\textbf{1} = \\textbf{0} \\\\\r\n    \\end{aligned}\r\n\\end{equation}\\]\r\nBecause the program above is not jointly convex, Dong et al. employ an alternating minimization scheme to solve the problem. Initially, set \\(Y = X\\) and we find a suitable \\(L\\) by solving \\[\\begin{equation}\r\n    \\begin{aligned}\r\n        \\min_L       & \\phantom{..} \\alpha \\text{tr}(Y^TLY) + \\beta ||L||_F^2, \\\\\r\n        \\textrm{s.t.}& \\quad \\text{tr}(L) = N,\\\\\r\n                     & \\quad L_{i,j} = L_{j,i} \\leq 0, \\phantom{..} i \\neq j, \\\\\r\n                     & \\quad L\\cdot \\textbf{1} = \\textbf{0}\r\n    \\end{aligned}\r\n\\end{equation}\\]\r\nNext, using \\(L\\) from the first step we solve \\[\\begin{equation}\r\n    \\begin{aligned}\r\n        \\min_Y \\phantom{..} ||X-Y||_F^2 + \\alpha \\text{tr}(Y^TLY)\r\n    \\end{aligned}\r\n\\end{equation}\\]\r\nBoth steps can be cast as convex optimazation problems. Specifically, the first problem can be solved with the method of alternating direction of multipliers (ADMM). The second can be solved algebraically. The model is applied to both synthetic and real world data, and compared to a technique used in machine learning that is similar to sparse inverse covariance estimation of Gaussian Markov Random Field models.\r\nPython Implementation\r\nThe program of Dong et. al. was originally implemented in MATLAB, but can be solved efficently using the free Python package CVXPY (found here). An implementation of the algorithm can be found here.\r\n\r\n\r\n\r\nFigure 1: An Erdos-Reyni Graph on 20 nodes, with edge probability 0.2. The ground truth network used to generate 100 synthetic gaussian signals for testing.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 2: The estimated network. Notice the networks share many features.\r\n\r\n\r\n\r\nThis graph was learned with no parameter optimization, but we can see on inspection they share several edges in common. Also, they have the same maximum and minimum degree (8 and 1) and the same total number of edges, \\(n = 38\\). Detailed results on the preformance of the algrithm with optimized parameters can be found in Dong et al. (2016) .\r\nContact Info\r\nGitHub\r\nEmail: skipmoses@gmail.com\r\n\r\n\r\n\r\nDong, Xiaowen, Dorina Thanou, Pascal Frossard, and Pierre Vandergheynst. 2016. “Learning Laplacian Matrix in Smooth Graph Signal Representations.” IEEE Transactions on Signal Processing 64 (23): 6160–73.\r\n\r\n\r\nKalofolias, Vassilis. 2016. “How to Learn a Graph from Smooth Signals.” In Artificial Intelligence and Statistics, 920–29. PMLR.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-11-gsp/gt_graph.png",
    "last_modified": "2022-05-31T12:55:36-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-22-linear-regression/",
    "title": "Linear Regression",
    "description": "A tutorial for Linear Regression using tidyverse.",
    "author": [
      {
        "name": "Skip Moses",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-03-22",
    "categories": [],
    "contents": "\r\nWhat is Linear Regression?\r\nOften the goal of data science is to make predictions from data. This will take the form of fitting our data to a model;\r\n\\[y = ax + b + \\epsilon\\]\r\nwhere \\(y\\) is our output/response variable, \\(x\\) is our input/predictor variable, \\(b\\) is a constant and \\(\\epsilon\\) is our irreducible error. Thus, if we have data \\(\\hat{y}\\) and \\(\\hat{x}\\), then fitting this data corresponds to finding a suitable \\(a\\) and \\(b\\).\r\n\r\n\r\n\r\nHere we have created some synthetic data. We can see there should be a line that can approximate this data (it was built to be so). But the question remains, how do we decide which line is best. The blue and green line seem better than the red line, but how do we quantify this?\r\nOne approach is to select the line that minimizes the aggregate distance from data points to our model. More rigorously, the line of best fit (our \\(a\\) and \\(b\\)) will be such the quantity\r\n\\[\\sum_{i=1}^N\\vert a\\hat{x} + b - \\hat{y}(\\hat{x}) \\vert \\]\r\nThe proof of the validity of this is beyond the scope of this tutorial.\r\nLinear Regression in R with tidyverse\r\nFortunately, there are several computer software packages capable of finding lines of best fit without any tedious calculations by hand. We will focus on the tidyverse implementation in r.\r\nThe first thing we need to do is install the package and load it.\r\n\r\n\r\n\r\nNext, we need to tell r we want to create a linear model using regression. Essentially, we a creating an interface for our model, that will work nicely with a wide range of packages.\r\n\r\n\r\n\r\nNow, we will specify an engine for fitting the model. This tells r what software should be used to fit the model. Some examples are Stan and glmnet, but we will simply use the ‘lm’ function.\r\n\r\nLinear Regression Model Specification (regression)\r\n\r\nComputational engine: lm \r\n\r\nLastly, in some cases the predictor does not represent a numeric outcome. In this case, we need to specify the the mode (regression or classification). Presently, since we are using linear regression, there is only one mode and we do not need to specify it.\r\nNow, that our model interface is created, we can use it to fit our data.\r\n\r\nparsnip model object\r\n\r\nFit time:  4ms \r\n\r\nCall:\r\nstats::lm(formula = yhat ~ x1, data = data)\r\n\r\nCoefficients:\r\n(Intercept)           x1  \r\n    120.764        4.605  \r\n\r\n\r\n[1] \"list\"\r\n\r\nNotice the output of fit is a list containing ‘spec’, ‘fit’, ‘preproc’ and ‘elapsed’. We will only deal with ‘fit’ in this tutorial.\r\nIn order to get more detailed picture of our model we can call the summary function. We use the ‘pluck()’ function to pull the ‘fit’ element from our output and create a summary.\r\n\r\n\r\nCall:\r\nstats::lm(formula = yhat ~ x1, data = data)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-38.57 -15.24   0.15  14.31  66.25 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 120.7644    15.2940   7.896 4.22e-12 ***\r\nx1            4.6047     0.2997  15.364  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 20.54 on 98 degrees of freedom\r\nMultiple R-squared:  0.7066,    Adjusted R-squared:  0.7037 \r\nF-statistic: 236.1 on 1 and 98 DF,  p-value: < 2.2e-16\r\n\r\nAlternatively, we can use the tidy() function to get a view our estimated parameters in a nice table.\r\n\r\n# A tibble: 2 × 5\r\n  term        estimate std.error statistic  p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)   121.      15.3        7.90 4.22e-12\r\n2 x1              4.60     0.300     15.4  7.60e-28\r\n\r\nIf we want to analyze the model statistics we call the glance() function.\r\n\r\n# A tibble: 1 × 12\r\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\r\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\r\n1     0.707         0.704  20.5      236. 7.60e-28     1  -443.  892.\r\n# … with 4 more variables: BIC <dbl>, deviance <dbl>,\r\n#   df.residual <int>, nobs <int>\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-22-linear-regression/linear-regression_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-05-31T12:55:26-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-09-my-name/",
    "title": "Extended Euclidean algorithm.",
    "description": "Sagemath code for an extended euclidean algorithm.",
    "author": [
      {
        "name": "Skip Moses",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-02-09",
    "categories": [],
    "contents": "\r\nFor my cryptography class we have code the Extended Euclidean Algrorithm. The Euclidean algorithm returns the greatest common divisor of two numbers.\r\nLet \\(a\\), \\(b \\in \\mathbb{Z}\\) be not both divisible by zero.\r\n\\[gcd(a,b) = \\text{ max}\\{d \\in \\mathbb{Z} : \\text{ } d\\vert a \\text{ and } d\\vert b \\}\\]\r\nIf we compute a \\(gcd\\) by hand we might proceed as follows assuming \\(b < a\\):\r\n\\[\\begin{align}\r\na &= bq_1 + r_1 &&0 \\leq r_1 \\leq b \\\\\r\nb &= q_2r_1 + r_2 &&0 \\leq r_2 \\leq r_1 \\\\\r\n\\vdots \\\\\r\nr_{n-2} &=  q_{n}r_{n-1} + r_n && 0 \\leq r_n < r_{n-1} \\\\\r\nr_{n-1} &=  q_{n+1}r_n + 0\r\n\\end{align}\\]\r\nFrom this process the last non zero \\(r_i\\) will be the gcd of \\(a\\) and \\(b\\).\r\nThis algorithm can be implemented with recursion as follows:\r\ndef my_gcd(a,b):\r\n     if a%b == 0:\r\n            return a\r\n     return my_gcd(b, a%b)\r\nGiven \\(a\\) and \\(b \\in \\mathbb{Z}\\) there is \\(S\\) and \\(T \\in \\mathbb{Z}\\) such that\r\n\\[\\begin{align}\r\na\\cdot S + b \\cdot T = \\text{gcd}(a,b)\r\n\\end{align}\\]\r\nWe need an iterative process to keep track of the \\(S\\) and \\(T\\) for the Extended Euclidean Algorithm.\r\n\\[\\begin{align}\r\nr_{i-2} &= S_{i-2}a + T_{i-2}b \\\\\r\nr_{i-1} &= S_{i-1}a + T_{i-1}\r\n\\end{align}\\]\r\nPlugin\r\n\\[\\begin{align}\r\nS_{i-2}a +T_{i-2}b = q_i\\left(S_{i-1}a + T_{i-1}b \\right) + r_i \\\\\r\nr_i = \\left(S_{i-2} - q_iS_{i-1}\\right) a + \\left(T_{i-2} - q_iT_{i-1}\\right) b\r\n\\end{align}\\]\r\nWhich gives\r\n\\[\\begin{align}\r\nS_i &= S_{i-2} - q_i S_{i-1} \\\\\r\nT_i &= T_{i-2} -q_i T_{i-1}\r\n\\end{align}\\]\r\n\\[\\begin{matrix}\r\na    &    b &   r & q & S  & T   \\\\\r\nx    &x     &   x & x & 1  & 0 \\\\\r\nx    &x     &   x & x & 0  & 1  \\\\\r\n2409 & 1023 & 363 & 2 & 1  & -2 \\\\\r\n1023 & 363  & 297 & 2 & -2 & 5  \\\\\r\n363  & 297  &  66 & 1 &  3 & -7 \\\\\r\n297  &  66  &  33 & 4 & -14& 33 \\\\\r\n66   &  33  &   0 & 2 &    &\r\n\\end{matrix}\\]\r\n\\(GCD = 33\\); \\(S = -14\\); \\(T = 33\\)\r\nThis process can be implemented as follows:\r\ndef Reset(S):\r\n   s = S[1]; S[0] = 1; S[1] = 0\r\n   return s\r\ndef Iterate(S, q):\r\n   s = S[0]\r\n   S[0] = S[1]\r\n   S[1] = s - q*S[1]\r\n   return S\r\ndef Extended_GCD(a, b, S = [1,0], T = [0,1]):\r\n   r = a%b\r\n   if r == 0:\r\n       return (b, Reset(S), Reset(T))\r\n   q = a//b\r\n   return Extended_GCD(b, r, Iterate(S,q), Iterate(T,q))\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-31T12:55:20-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-07-matlab/",
    "title": "MATLAB",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Skip Moses",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-02-07",
    "categories": [],
    "contents": "\r\nOverview\r\nToday I am going to attempt to learn MATLAB (Matrix Laboratory). MATLAB is a matrix based language geared towards engineers and scientists. With MATLAB one can\r\nAnalyze Data\r\nDevelop Algorithms\r\nCreate models and applications\r\nMy Goals\r\nFamilarize myself with basic syntax.\r\nLearn how to write simple loops and functions.\r\nLearn how to import and use scripts.\r\nLearn how to write a matrix to a text file.\r\nI need MATLAB so I can run the algorithm from Dong et. al. and compare results with my version that is implemented in python.\r\nMy hope is learning basic MATLAB will give me a clearer understanding of the algorithm.\r\nPlan\r\nI will watch this video, and follow along to accomplish goal 1.\r\nI will write some simple loops and functions for goal 2.\r\nI will put these functions in a script and attempt to run it.\r\nI will get Dong et. al. algorithm running.\r\nNotes\r\nOpening MATLAB for the first time\r\nWhen you open MATLAB most of the screen will be occupied by the Command Window. This functions much like the terminal with python. Here you can input simple commands. The Current Folder will show your current working directory. Notice this is very similar to RStudio. The Workspace window will show values of variables. The PLOT tab is where graphics will be displayed, and the APP tab will show different apps you can add (similar to adding packages in R).\r\nThe Command Window and Workspace will be the most important.\r\nSome common operations\r\nThe basic arithmetic opperations for python are +, -, *, and /. The ‘=’ sign is used for assignment.\r\ndouble clicking the top of a window will enlarge and minimize it.\r\nIf we enlarge the **Workspace*, we can right click (control click on macbook) and select several different properties to be displayed.\r\nWe can clear our command window by running the command ‘clc’. We can in turn hit the up arrow key to scroll through all previous commands that have been run in a given session.\r\nRunning the command ‘whos’ will display the workspace.\r\nIf we want to clear our workspace we can use the command ‘clear all’. We can delete a single variable, ‘a’, by the command ‘clear a’.\r\nUnderstanding Variables\r\nBy default, every variable defined in MATLAB will be a matrix. When we assign a single integer to a variable, MATLAB stores this as \\(1\\times 1\\) matrix.\r\nWe can create a matrix of arbitrary size by:\r\n‘a = [ 9 8 8;]’\r\nwill creat a \\(1\\times 3\\) matrix.\r\n‘a = [ 1 1 1; 2 2 2; 3 3 3;]’\r\nwill create the matrix:\r\n\\[\\begin{bmatrix}\r\n1&1&1\\\\\r\n2&2&2\\\\\r\n3&3&3\\\\\r\n\\end{bmatrix}\\]\r\nIf we now expand our workspace, we can click on our variable ‘a’, and see the matrix displayed in an excel-like grid. We can now use some of the other attributes in our workspace like min, max, mean, median, ect.\r\nWe can right click our variable in our workspace and preform some useful operations like; rename it, change its value, duplicate it and make various plots if its is the correct size.\r\nWe can also save the variable as .mat file.\r\nWe can load a .mat file with the ‘load(FILENAME.mat)’.\r\nDifferent Types of Variables\r\nSo far, we have only used numeric variables. We can have character variables char. The command\r\na = ‘hello world’\r\nwill create a \\(1\\times 11\\) character matrix.\r\nWe can add a row to matrix, a, with the following:\r\na(2,:) = a(1,:)\r\nThis will output\r\n‘hello world’\r\n‘hello world’\r\nNote: you must make sure the rows are the same size when adding them\r\nWe can get around this by using strings.\r\nWe can create an empty string with:\r\na = [string(‘Hi how ar you’) string(‘hello world’)]\r\noutput:\r\na =\r\n1×2 string array\r\n\"Hi how ar you\"    \"hello world\"\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-31T12:55:14-07:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Today I am Learning",
    "description": "Welcome to our new blog, Today I am Learning. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Skip Moses",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-02-07",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-31T12:55:46-07:00",
    "input_file": {}
  }
]
